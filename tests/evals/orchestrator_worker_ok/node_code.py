from state_code import State
from typing import Optional, Dict, Any
import questionary
from langchain_core.runnables.config import RunnableConfig
from lgcodegen_llm import node_chat_model, worker_chat_model

def human_display(val):
    print(val)

# Node Function
def orchestrator(state: State, *, config: Optional[RunnableConfig] = None) -> Dict[str, Any]:
    # reads: topic
    # writes: sections
    
    topic = state.topic
    
    # If topic is not provided, prompt the user for one
    if not topic:
        topic = questionary.text("What report topic would you like to generate?").ask()
    
    # Create the LLM for generating sections
    llm = node_chat_model()
    
    # Generate the sections based on the topic
    generate_sections_prompt = f"""Generate up to 4 descriptive section titles suitable for a report on {topic}.
Each section title should be informative and help structure a comprehensive report.
Return only the section titles, one per line."""
    
    sections_response = llm.invoke(generate_sections_prompt)
    
    # Parse the response into a list of sections
    sections = [section.strip() for section in sections_response.content.strip().split('\n') if section.strip()]
    
    # Ensure we have no more than 4 sections
    sections = sections[:4]
    
    # Display the generated sections to the user
    human_display(f"Generated {len(sections)} sections for report on '{topic}':")
    for i, section in enumerate(sections, 1):
        human_display(f"{i}. {section}")
    
    return {"topic": topic, "sections": sections}

# Node Function
def worker(field_value: str, *, config: Optional[RunnableConfig] = None) -> Dict[str, Any]:
    # Note: This is a worker function which gets a single section title as input
    # rather than the full state
    
    # Get the topic and worker_index from the config (passed by the worker node infrastructure)
    topic = config.get("state", {}).get("topic", "")
    section = field_value
    
    # Get worker_index from metadata (or default to 0)
    metadata = config.get("metadata", {})
    worker_index = metadata.get("worker_index", 0)
    
    # Create the LLM for generating section content based on worker_index
    llm = worker_chat_model(worker_index)
    
    # Include LLM info in the output for tracking
    llm_info = llm.__class__.__name__
    if hasattr(llm, 'model'):
        llm_info += f" ({llm.model})"
    
    # Generate content for this section
    generate_content_prompt = f"""Generate detailed and informative content for the section titled "{section}" 
in a report about "{topic}".

The content should be comprehensive yet concise, providing valuable information relevant to this specific section.
Format the content with proper paragraphs and structure.
"""
    
    content_response = llm.invoke(generate_content_prompt)
    section_content = content_response.content.strip()
    
    # Return the processed section to be added to processed_sections
    return {"processed_sections": [f"## {section} (Generated by {llm_info})\n\n{section_content}"]}

# Node Function
def synthesizer(state: State, *, config: Optional[RunnableConfig] = None) -> Dict[str, Any]:
    # reads: processed_sections
    # writes: (final output is displayed to user)
    
    topic = state.topic
    processed_sections = state.processed_sections
    
    # Create a title for the report
    report_title = f"# Report: {topic}"
    
    # Combine all sections into one cohesive report
    full_report = f"{report_title}\n\n"
    full_report += "\n\n".join(processed_sections)
    
    # Display the final report to the user
    human_display("\n--- FINAL REPORT ---\n")
    human_display(full_report)
    human_display("\n--- END OF REPORT ---\n")
    
    # We're not updating any state field, just displaying the output
    return {}